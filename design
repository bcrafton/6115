
> how do we structure this.
  > do not want conv class.
  > probably just want PimCore.

> but then we need to accumulate from all the PimCores.
  > so makes little sense to use model.forward.
  > more convenient to store list of conv/dense layers inside of PimCore so we can just call forward.
  
> how do we slice this all up ? 
  > for each (normal) layer, cut it up 16 ways, give each slice layer its own slice.

> make it work, then make it good.

----

we still have to do pim convolution
> we are still doing regular convolution not bit-by-bit pim convolution
> should have both options to compare throughput.

----

okay so we only need to move the activations around
and we need to send them in packets
we cud start with just sending the weights ? 
but i mean we need to consider the size of the bus right ? 
... yeah we care about flits really
we can assume everything is 8 bit words ... nope accumulated convs probably 16. 

can we ignore packets to start ? 
just count the number of things we send ? 
the scatter and gather right now is whack lol.

so at the core level we just have "rec, send" 
but we dont really consider injection points
or sim as if it is a mesh
where would injection points even be ?

wudnt injection points just be SRAM ? 
that wud be interleaved between the cores ? 
yeah 
alright so that will get a bit tricky then. 

----

so we need to create a mesh with SRAM and PIM cores.
128x128 cores I guess ? can we find SRAM blocks of similar size and make it all work ?
yeah but they will probably need to be double the size or something.
thats fine.

so main memory feeds into sram
sram feeds pim cores
pim cores back to sram
sram back to main memory
yeah other thing would just be sticking SRAM in the PIM cores.
which we should probably do instead.

alright so they we have to use local reuse and shit. 
so i guess bottom line will be
how to do MAERI for PIM.

so where does main memory come from ?
well if we have a package, then data can go to all of them, right ? 
yeah ... not sure about that
is that the big question tho ? 

----

FLIP CHIP VS WIRE BONDING.

MM -> PIM.SRAM -> PIM.PIM -> PIM.SRAM -> MM
> how does it get to PIM.SRAM ? 
> package have pins to all cores ? 

lets assume we can communicate with all cores. 
> then we need to come up with a gather, scatter, reduce technique.

still need to come up with a mapping stratedgy to go from feature map -> core
> like some dictionary or 2d list or something

----

> we communicate with 2x2 core blocks.
> so we can probably share SRAM in this range to a single large bank.
nah dont share SRAM there actually. 
maybe just do (L1, L2) caches ? 

so we have maps from network to block
then from block to core ? 
maybe something like that.

----

okay, so finishing these ideas up,
how do we implement ? 
we want to send different data to each core.
then gather them all together i think.

scatter:
so the scatter will be sending to each block
then the block will distribute to each core

gather:
each core to each block
each block does a tree.

we start by just implementing 
the split
then reducing it ? 

yeah its tough we really just need to do one part at a time .
not sure where to start.

----

so its:
map mm -> block
map block -> core
then perform all reduce.

> so i think there is an easier way to do this mapping
divide the data up into chunks

> transpose + reshape->(16, ...)
  then send each block its chunk ...

this will work for now I think, 
> but might cause problems in future when we consider that we cant send all the data at once.
> OR we want to send it by XY coords not channel coords.

i think for now we leave scatter as is ...

----

lets do gather.
so we basically call reduce.
can we do this object oriented for each core with pointers ? 
> we cud by having each core call its neighbors based on id.
> each core knows what part of all reduce it is.

yeah but wont every node have to return something ? 
yeah ...

----

alright so we have a gather implementation.
i suppose we want to implement the pim operations now.
basically sending bits instead of 8 bit numbers.

----

pim operation is sorta done
i think the next step is figuring out the tile thing
because that is what has to do the (b, q, relu)

yeah so i guess all tiles handle that shit. 

array
tile
core 

he has the pool/activation at the core level.

----

> turn everything into tiles ? 
  > that all do the same thing ? 

how many tiles do we divide things up into ? 
> 128x128 -> 128x16
> up to 256 outputs.
> 256 / 16 -> 16.
  > means we need 16 tiles for this layer.
  > fine with this being the width I think.
  > maybe we wud want less tiles tho.
  >> duplications add up in the paper if u check that part out.

so really what we want to do is conv.

WAIT:
it goes:
> chip - ?
> tile - 9
> pe - 16
> array - 128x128

----

alright so we divide up conv layer into 1 PE = 16 arrays.
128/8 = 16. 256/16 = 16.

then what do we do ? 
i dont think we shud worry about how we distribute weights such that we maximize throughput
just assume that we always can.
well the only thing is - we probably want to distribute so we have multiple convolutions over 2 subarrays and then combine them together. 

yeah but if we are building a large scale design we dont really care that much ... 
because u parallelize over BXY.

----

okay, so the way i see it. 
we shud pull back from PIM operations
and figure out the architecture first.

actually dont need PIM to do this .
















