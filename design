
> how do we structure this.
  > do not want conv class.
  > probably just want PimCore.

> but then we need to accumulate from all the PimCores.
  > so makes little sense to use model.forward.
  > more convenient to store list of conv/dense layers inside of PimCore so we can just call forward.
  
> how do we slice this all up ? 
  > for each (normal) layer, cut it up 16 ways, give each slice layer its own slice.

> make it work, then make it good.

----

we still have to do pim convolution
> we are still doing regular convolution not bit-by-bit pim convolution
> should have both options to compare throughput.

----

okay so we only need to move the activations around
and we need to send them in packets
we cud start with just sending the weights ? 
but i mean we need to consider the size of the bus right ? 
... yeah we care about flits really
we can assume everything is 8 bit words ... nope accumulated convs probably 16. 

can we ignore packets to start ? 
just count the number of things we send ? 
the scatter and gather right now is whack lol.

so at the core level we just have "rec, send" 
but we dont really consider injection points
or sim as if it is a mesh
where would injection points even be ?

wudnt injection points just be SRAM ? 
that wud be interleaved between the cores ? 
yeah 
alright so that will get a bit tricky then. 

----

so we need to create a mesh with SRAM and PIM cores.
128x128 cores I guess ? can we find SRAM blocks of similar size and make it all work ?
yeah but they will probably need to be double the size or something.
thats fine.

so main memory feeds into sram
sram feeds pim cores
pim cores back to sram
sram back to main memory
yeah other thing would just be sticking SRAM in the PIM cores.
which we should probably do instead.

alright so they we have to use local reuse and shit. 
so i guess bottom line will be
how to do MAERI for PIM.

so where does main memory come from ?
well if we have a package, then data can go to all of them, right ? 
yeah ... not sure about that
is that the big question tho ? 

----

FLIP CHIP VS WIRE BONDING.

MM -> PIM.SRAM -> PIM.PIM -> PIM.SRAM -> MM
> how does it get to PIM.SRAM ? 
> package have pins to all cores ? 

lets assume we can communicate with all cores. 
> then we need to come up with a gather, scatter, reduce technique.

still need to come up with a mapping stratedgy to go from feature map -> core
> like some dictionary or 2d list or something

----

> we communicate with 2x2 core blocks.
> so we can probably share SRAM in this range to a single large bank.
nah dont share SRAM there actually. 
maybe just do (L1, L2) caches ? 

so we have maps from network to block
then from block to core ? 
maybe something like that.

----

okay, so finishing these ideas up,
how do we implement ? 
we want to send different data to each core.
then gather them all together i think.

scatter:
so the scatter will be sending to each block
then the block will distribute to each core

gather:
each core to each block
each block does a tree.

we start by just implementing 
the split
then reducing it ? 

yeah its tough we really just need to do one part at a time .
not sure where to start.

----

so its:
map mm -> block
map block -> core
then perform all reduce.

> so i think there is an easier way to do this mapping
divide the data up into chunks

> transpose + reshape->(16, ...)
  then send each block its chunk ...

this will work for now I think, 
> but might cause problems in future when we consider that we cant send all the data at once.
> OR we want to send it by XY coords not channel coords.

i think for now we leave scatter as is ...

----

lets do gather.
so we basically call reduce.
can we do this object oriented for each core with pointers ? 
> we cud by having each core call its neighbors based on id.
> each core knows what part of all reduce it is.

yeah but wont every node have to return something ? 
yeah ...

----

alright so we have a gather implementation.
i suppose we want to implement the pim operations now.
basically sending bits instead of 8 bit numbers.

----

pim operation is sorta done
i think the next step is figuring out the tile thing
because that is what has to do the (b, q, relu)

yeah so i guess all tiles handle that shit. 

array
tile
core 

he has the pool/activation at the core level.

----

> turn everything into tiles ? 
  > that all do the same thing ? 

how many tiles do we divide things up into ? 
> 128x128 -> 128x16
> up to 256 outputs.
> 256 / 16 -> 16.
  > means we need 16 tiles for this layer.
  > fine with this being the width I think.
  > maybe we wud want less tiles tho.
  >> duplications add up in the paper if u check that part out.

so really what we want to do is conv.

WAIT:
it goes:
> chip - ?
> tile - 9
> pe - 16
> array - 128x128

----

alright so we divide up conv layer into 1 PE = 16 arrays.
128/8 = 16. 256/16 = 16.

then what do we do ? 
i dont think we shud worry about how we distribute weights such that we maximize throughput
just assume that we always can.
well the only thing is - we probably want to distribute so we have multiple convolutions over 2 subarrays and then combine them together. 

yeah but if we are building a large scale design we dont really care that much ... 
because u parallelize over BXY.

----

okay, so the way i see it. 
we shud pull back from PIM operations
and figure out the architecture first.

actually dont need PIM to do this .

----

yeah okay the new structure is the challenge here.
when u cut the conv, what happens ? 
> u get PE/TILE
> then u pass it back to Model which does more hacky shit

okay so its clear
need to fix this garbage structure. 
i like the idea of doing it the same way we do it in the RTL
  where the weights are in the crossbar
  and we have instructions that handle shit.
  
hard to visualize this now.
> stop thinking about current structure and think about the correct one.

have 6 conv layers
need to spread them over the arrays
BUT this means we need the distributed convolution function. 

what we do right now:
> split each conv 16 ways
> put 1/16 of all 6 layers into a array
> not a terrible idea but just dosnt work for what we want to do.

so if we try to fit 6 layers into 1 128x128 array, we will not have enof room for row skipping. 
so im thinking we shud use bigger arrays. 

> yeah they use pipelining, that makes sense.
  > they even say it dosnt make sense to mix layers.
  > well it does if u are 1024x1024.
  
----

yeah def dosnt make sense at 128x128
alright so lets not mix layers.

so each layer will get its own list of PEs. 
> do we really want to build it like that tho ? 
> i think we shud build it to handle mixing.

where each array stores layers 
where total weights in all layers sums to 128x128
yeah i like that idea.

this means that network should perform convolution right ?
ah that is pretty cool actually.

----

there is nothing to conv here, 
the only thing with conv is flattening the patch out and doing 1 patch at a time.

so we will do the conv in network
how do we create the arrays though ? 
do we do it from Conv ? 

the issue with creating arrays, is if they are taller than 128.
what do we do with a dense layer ? 
something that is 1024x10.

i guess it comes down to whether the PEs should sum together OR they should concatenate.
because i wud like to do this as clever distributed arrays
the network has mappings to go gather shit at each one. 

but then there is the issue with knowing how the x data shud feed into each one.

----

alright so the issue was with doing this correctly - lol
is that we need to slice things up into N 128x128 arrays. 

previously we sliced things up 4 ways and then appended to each PCM
that worked because we had 1024 rows.
now we only have 128 per.

so now we need to change things up a bit with how we map weights.
3*3*64/16 = 36. 128-36 = 100, dont want to throw all those rows away

so how can we make use of them ?
and we dont want to process by layer.
we want to be configurable.
also tho - there is no way we wud have ADC space for this.

its supposed to be 8 columns to an ADC.
so we wud definitely want to use more than 1 array.
how do we use more than 1 column per array.

----

so i guess the question is, what if we have 32 arrays or something ? 
do we go to -> 3*3*64/32 ... thats basically nothing. 

see that is the problem there. 
u need to come up with a rule for this case. 

and what if the height is larger than 128 ? 
well then u need to add some together
and if its shorting u need to bin them together

----

> so both layers spit out a matrix
> then we add distributed conv and distriuted dense to network
> and we have to dice up that matrix somehow - but the idea remains the same it will always be matrix multiplication
  > which greatly simplifies things. 
  
besides for layers, does it ever make sense to split rows ? 
or will the weights always be active anyways ?
well no, it does make sense so that u can run in other arrays. 
but how much u split is the question
AND then u have to sum between the arrays. but u already have to do that anyways so i wudnt worry about that
well it cud be advantagous not to have to sum accross all of them tho.

----

back to previous idea
we get a matrix 
okay time to slice it up.
for our purposes just slicing up within the PE.

becaue we have 128x128, so wud we want to slice up over many PEs ?
> that is the big q

16 array
9 PE
16 Tile

okay - lets start by turning everything into PEs 
we can think about splitting over layers later.

> time to make progress.

----

alright so we dump weights
then we divide up into arrays.
3x3x128x128 is larger than we can stick into 1 array.
do we put it into more than 16 arrays ? 

I think we maybe ignore the PE thing they do
and just do 2 abstraction layers
Array and whole thing. 
ehh i actually like the PE thing
okay maybe just 3 tho.

yeah so - Array, PE, Whole chip
well actually what they do with the 3x3 filter is pretty clever lol.

so we divide over 9 tiles - that much i like.
but what do they do about layers like
3x3x64x64.

oh wait
they duplicate inside the same PE dont they.
and then they run many patches in the same PE.
i see.

----

what happens for dense layers lol
can we just ignore dense layers ? 
I think we just ignore dense layers ...

that will make life simpler.

or we scrap the Tile thing. 

they do 16*128x128 so they can do a 1x1x512.

----

i dont have a problem with slicing by 1x1s, but we shud toss the idea of tile=3x3 PE I think.

----

> u dont need to slice over the 3x3 in this way.
> but does it make sense to do so ? 

i mean keep in mind we are doing this for large chip
almost feel like it wud make the most sense to scale the #MAC to how much duplication u do.

so this wud be easiest if we knew the size of the chip.
> 1560 PEs on whole chip
> 1560 * 16 * 128x128
> that is insane.

but maybe what we shud target something else. 
how many of these PEs ? 

well actually we shud be able to scale up.
and just duplicate shit.
and duplicate based on what DOSNT create a bottleneck.

----

true - but realize that we still need to create a kernel for each one
before it can be duplicated.

OH WOW - so this is interesting because of duplication.
u need pretty crazy SRAM bandwidth.

---- 

so how you duplicate / slice (by rows) matrices will depend on how much space we have ? 
well duplcation yes, but different for slice
slice needs to know how much space we have before hand
and what slice size we want. 
because if u only have enof space for this layer then u are in trouble. 
well i guess it really just needs to know how many rows per array u want to do .

yeah thats really it.

and instead of slicing channel wise, we shud be slicing XY wise.
WHICH is actually what we started doing in the RTL recall.

----

alright so we starting to get stuff now.
so first we create a kernel for each layer
then we duplicate based on how much space we left
duplication is ez and deterministic.

1) rows per array
2) 
3) duplicate based on leftover space and mac in each layer.

----

shud we not compute rows per array ? 
i think rows per array can certainly be computed
BUT I dont want to do it right now.
so we will hardcod it.

----

can we use splitting in XY dimension, to prevent weird shapes ? 
> split XY, 
> yeah i mean we can just make conv do that. 
  > but consider case for 3x3x64x64.
  > we cant satisfy 128 rows if we split the 9 ways, so we end up with less
  > actually seems to be no reason to me to split XY direction.
  > just fill it all the way up.

----

okay so now we have done a good job splitting things up BUT we have a problem.
network cannot do a conv.
so, i dont think we have a network in the first place.
well i take that back actually.

the issue with splitting up the way u describe, is then u cant mix layers.
so im thinking there are two ways to go:
> collect all arrays in model, then pass them back.
> or bundle all params into a dictionary and return them from conv.

----

> distributed conv
> duplicate weights
  > 2048 arrays = 128 PE * 16 Arrays

----

okay we sorta did that ^
now what ?

cycle level accuracy ? 
> x bit sharing
  > any reason not to group arrays together ? 
    > ArrayGroup ? 
  
> adc < col
  > 8 adc / col

> pim dosnt do a full 128 bits/cycle.
  > assume 0 skipping and just return np.count_ones(x) / 8

----

okay we added all of these, and SRAM came crashing down lol.
will it really require this many cycles ? 

----

our contribution might just be :
> link sizing
> placement
> group sizing 
> providing bandwidth as a function of (X, W)

> large groups may have small input link, but will still have large output link ...

----

128 * 128 / (8 COL/ADC) / (8 WBIT) / (8 XBIT) = 32 MAC / CYCLE best case
> oh wait they do 2 bit / cell
> have to double this.
> and we are not certain what an op is ...

25k arrays * 32 MAC/cycle * 100e6 Hz = 80TOPS
they claim 20TOPS/W so perhaps it just consumes 4W power ? 

problem is that total ADC energy = 22 * 1e-12 * 25000 * 100e6 = 55 J

----

Table II shows the component parameters of the proposed
architecture. The energy is given in energy per unit (or module
and PE) per operation (or bit). For example, the energy of
ADC is 22.45 pJ/op, it is the energy for all the ADCs in
one sub-array to process one operation, here the operation
means once the sub-array is activated and given a vector of
input pulse, the ADC will get one group of outputs from
the sub-array. It should be noted that, considering about the
area overhead, we have grouped the columns and shared every
8 columns with one ADC. In order to get the outputs of all
the columns in one sub-array, the ADCs need to work 8 oper-
ations. Moreover, since the input precision is set to be 8-bit,
the entire sub-array need to operate 8 times, and meanwhile
these outputs will be shifted and accumulated. 

In addition,
in the PE level, the sub-array energy is 25.04 nJ/op, which
tells the total energy for one single sub-array to do one
8-bit vector-matrix multiplication (8-bit inputs multiply 8-bit
weights and accumulate the dot-products). This includes all
the operations in sub-array level as discussed, such as sharing
and switching the ADCs among columns and shift-adds for
high-precision inputs.

----

25.04 * 1e-9 / 8 / 8 * 25000 * 100e6
nah they dont specify a clock ...
we cud setup some equations to solve this tho.

----

our PE are wide, not tall. so they all require the same banwidth right ? 

each PE in our design requires
128 WLs * 8 XBIT / (8 XBIT * 8 COL) = 16 bit/cycle
16b * 275 PE = 136 32b SRAMs ...
scale up by 10 = 1360 SRAM.
0.05 * 1360 = 68mm2 of SRAM ... still quite a bit ...

Each of their PE = 4Kb @ 128 bit width
which they claim is 0.0073 mm2 ...
0.044 ... wtf ??

----

why do we have 2k array, 275 pe ? 
why do we ever have a 16 ? 
16 * 128 / 8 = 256. oh lol.

2044 / 275 = 7.4
still seems kinda high ...
nah actually there is (4,8,16)

----

> were going to email.
> topology
> actually sending packets
> gather scatter
> link scaling

---- 

> it seems like all of these are dependent on figuring out some NoC structure
> then we can send packets, do gather/scatter ect.

> how do we do an actual topology
  > each pe is a node in network ? 
    > thats a lot of nodes.

we can group more together, not like we need the bandwidth. 

----

so we dont have a great idea of what to do yet, BUT still several more things to implement, so might as well do that. 

----






























