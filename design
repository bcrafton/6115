
> how do we structure this.
  > do not want conv class.
  > probably just want PimCore.

> but then we need to accumulate from all the PimCores.
  > so makes little sense to use model.forward.
  > more convenient to store list of conv/dense layers inside of PimCore so we can just call forward.
  
> how do we slice this all up ? 
  > for each (normal) layer, cut it up 16 ways, give each slice layer its own slice.

> make it work, then make it good.

----

we still have to do pim convolution
> we are still doing regular convolution not bit-by-bit pim convolution
> should have both options to compare throughput.

----

okay so we only need to move the activations around
and we need to send them in packets
we cud start with just sending the weights ? 
but i mean we need to consider the size of the bus right ? 
... yeah we care about flits really
we can assume everything is 8 bit words ... nope accumulated convs probably 16. 

can we ignore packets to start ? 
just count the number of things we send ? 
the scatter and gather right now is whack lol.

so at the core level we just have "rec, send" 
but we dont really consider injection points
or sim as if it is a mesh
where would injection points even be ?

wudnt injection points just be SRAM ? 
that wud be interleaved between the cores ? 
yeah 
alright so that will get a bit tricky then. 

----

so we need to create a mesh with SRAM and PIM cores.
128x128 cores I guess ? can we find SRAM blocks of similar size and make it all work ?
yeah but they will probably need to be double the size or something.
thats fine.

so main memory feeds into sram
sram feeds pim cores
pim cores back to sram
sram back to main memory
yeah other thing would just be sticking SRAM in the PIM cores.
which we should probably do instead.

alright so they we have to use local reuse and shit. 
so i guess bottom line will be
how to do MAERI for PIM.

so where does main memory come from ?
well if we have a package, then data can go to all of them, right ? 
yeah ... not sure about that
is that the big question tho ? 

----

FLIP CHIP VS WIRE BONDING.

MM -> PIM.SRAM -> PIM.PIM -> PIM.SRAM -> MM
> how does it get to PIM.SRAM ? 
> package have pins to all cores ? 

lets assume we can communicate with all cores. 
> then we need to come up with a gather, scatter, reduce technique.

still need to come up with a mapping stratedgy to go from feature map -> core
> like some dictionary or 2d list or something

----

> we communicate with 2x2 core blocks.
> so we can probably share SRAM in this range to a single large bank.
nah dont share SRAM there actually. 
maybe just do (L1, L2) caches ? 

so we have maps from network to block
then from block to core ? 
maybe something like that.

----

okay, so finishing these ideas up,
how do we implement ? 
we want to send different data to each core.
then gather them all together i think.

scatter:
so the scatter will be sending to each block
then the block will distribute to each core

gather:
each core to each block
each block does a tree.

we start by just implementing 
the split
then reducing it ? 

yeah its tough we really just need to do one part at a time .
not sure where to start.

----

so its:
map mm -> block
map block -> core
then perform all reduce.

> so i think there is an easier way to do this mapping
divide the data up into chunks

> transpose + reshape->(16, ...)
  then send each block its chunk ...

this will work for now I think, 
> but might cause problems in future when we consider that we cant send all the data at once.
> OR we want to send it by XY coords not channel coords.

i think for now we leave scatter as is ...

----

lets do gather.
so we basically call reduce.
can we do this object oriented for each core with pointers ? 
> we cud by having each core call its neighbors based on id.
> each core knows what part of all reduce it is.

yeah but wont every node have to return something ? 
yeah ...

----

alright so we have a gather implementation.
i suppose we want to implement the pim operations now.
basically sending bits instead of 8 bit numbers.

----

pim operation is sorta done
i think the next step is figuring out the tile thing
because that is what has to do the (b, q, relu)

yeah so i guess all tiles handle that shit. 

array
tile
core 

he has the pool/activation at the core level.

----



























